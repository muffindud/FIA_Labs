{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Fundamentals of Artificial Intelligence***\n",
    "\n",
    "> **Lab 6:** *Natural Language Processing and Chat Bots* <br>\n",
    "\n",
    "> **Performed by:** *Corneliu Catlabuga*, group *FAF-213* <br>\n",
    "\n",
    "> **Verified by:** Elena Graur, asist. univ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from warnings import filterwarnings\n",
    "\n",
    "MODEL = 't5-base'\n",
    "SOURCE_LEN = 512\n",
    "TARGET_LEN = 512\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "\n",
    "Set up the Telegram Bot. Interact with BotFather on Telegram to obtain an API token. Create your Telegram Bot (its name should follow the pattern FIA_Surname_Name_FAF_21x). Make sure you are able to receive and send requests to it.\n",
    "\n",
    "1. Bot link: [FIA_Catlabuga_Corneliu_FAF_213](https://t.me/fiacorneliucatlabugafaf213bot)\n",
    "\n",
    "2. Run `app.py` to start the bot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "\n",
    "Create a dataset that will serve as a training set for your model. It should follow the rules:\n",
    "- an entry consists of two parts: the question and the answer;\n",
    "- there are at least 75 entries written by you in your dataset;\n",
    "- questions should be something tourists or locals can ask about a new city.\n",
    "\n",
    "You can increase your dataset by adding open-source data. However, you MUST clearly show the questions written by you. Split your dataset into train and validation.\n",
    "\n",
    "*Hint: it is recommended to split it into 80% and 20%, but you can adjust it according to your needs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset.csv')\n",
    "\n",
    "questions = dataset['question'].tolist()\n",
    "answers = dataset['answer'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "\n",
    "Use Tensorflow or Pytorch to implement the architecture of the Neural Network you are planning to use. It is highly recommended to use a Seq2Seq model (implement an LSTM or GRU architecture). You are NOT allowed to use pre-built or existing solutions (yep, connecting to GPT will not work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "def tokenize(data, max_len):\n",
    "    return tokenizer(data, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs['input_ids'][idx],\n",
    "            'attention_mask': self.inputs['attention_mask'][idx],\n",
    "            'labels': self.targets['input_ids'][idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, questions_val, _, answers_val = train_test_split(questions, answers, test_size=0.2)\n",
    "\n",
    "tokenized_questions_train = tokenize(questions, SOURCE_LEN)\n",
    "tokenized_answers_train = tokenize(answers, TARGET_LEN)\n",
    "train_dataset = Seq2SeqDataset(tokenized_questions_train, tokenized_answers_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Save the used tokens to later check if the question is according to the tokens\n",
    "torch.save(tokenized_questions_train, 'models/used_tokens.pt')\n",
    "\n",
    "tokenized_questions_val = tokenize(questions_val, SOURCE_LEN)\n",
    "tokenized_answers_val = tokenize(answers_val, TARGET_LEN)\n",
    "val_dataset = Seq2SeqDataset(tokenized_questions_val, tokenized_answers_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "\n",
    "Train your model and fine-tune it based on the chosen performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, model_name=MODEL):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs: int = 10, file_name: str = 'model.pth'):\n",
    "    model = Seq2SeqModel().to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loop = tqdm(train_dataloader, leave=True)\n",
    "\n",
    "        for batch in loop:\n",
    "            loop.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    torch.save(model.state_dict(), f'models/{file_name}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    total_bleu = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(val_loader, leave=True)\n",
    "        for batch in loop:\n",
    "            loop.set_description(\"Evaluating: \")\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model.model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=TARGET_LEN)\n",
    "            predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            references = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            for p, r in zip(predictions, references):\n",
    "                total_bleu += sentence_bleu([r.split()], p.split())\n",
    "                num_samples += 1\n",
    "\n",
    "    return total_bleu / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 8/8 [00:15<00:00,  1.94s/it, loss=2.3] \n",
      "Epoch 1: 100%|██████████| 8/8 [00:13<00:00,  1.71s/it, loss=1.35]\n",
      "Epoch 2: 100%|██████████| 8/8 [00:08<00:00,  1.10s/it, loss=1.11]\n",
      "Epoch 3: 100%|██████████| 8/8 [00:09<00:00,  1.22s/it, loss=0.859]\n",
      "Epoch 4: 100%|██████████| 8/8 [00:09<00:00,  1.22s/it, loss=0.729]\n",
      "Epoch 5: 100%|██████████| 8/8 [00:09<00:00,  1.22s/it, loss=0.622]\n",
      "Epoch 6: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it, loss=0.399]\n",
      "Epoch 7: 100%|██████████| 8/8 [00:09<00:00,  1.22s/it, loss=0.264]\n",
      "Epoch 8: 100%|██████████| 8/8 [00:09<00:00,  1.22s/it, loss=0.259]\n",
      "Epoch 9: 100%|██████████| 8/8 [00:10<00:00,  1.26s/it, loss=0.228]\n",
      "Evaluating: : 100%|██████████| 2/2 [00:00<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.6664665989118275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TRAINED_MODEL = 'model10base.pth'\n",
    "\n",
    "model = train(10, TRAINED_MODEL)\n",
    "bleu_score = evaluate(model, val_dataloader)\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "\n",
    "Integrate your model into your Telegram ChatBot, so that the sent messages are taken as input by the model and its output is sent back as a reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tokens(tokens):\n",
    "    saved_tokens = torch.load('models/used_tokens.pt')['input_ids']\n",
    "    return all([t in saved_tokens for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12838,    31,     7,     1])\n",
      "12838: Andy: False\n",
      "31: ': True\n",
      "7: s: True\n",
      "1: </s>: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Invalid tokens'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Seq2SeqModel().to(device)\n",
    "model.load_state_dict(torch.load(f'models/{TRAINED_MODEL}'))\n",
    "\n",
    "def generate_answer(question, model, tokenizer):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    if not check_tokens(input_ids[0]):\n",
    "        print(input_ids[0])\n",
    "\n",
    "        for token in input_ids[0]:\n",
    "            print(f\"{token}: {tokenizer.decode(token)}: {check_tokens([token])}\")\n",
    "\n",
    "        return \"Invalid tokens\"\n",
    "\n",
    "    output = model.model.generate(input_ids, max_length=TARGET_LEN)\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return decoded\n",
    "\n",
    "# Test example\n",
    "test_question = \"Andy's\"\n",
    "display(generate_answer(test_question, model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Telegram Bot\n",
    "\n",
    "(Code can be found in `app.py` and `utils.py`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*utils.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "MODEL = 't5-base'\n",
    "TRAINED_MODEL = 'model10base.pth'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    # Model class\n",
    "    ...\n",
    "\n",
    "model = Seq2SeqModel().to(device)\n",
    "model.load_state_dict(torch.load(f'models/{TRAINED_MODEL}'))\n",
    "\n",
    "def check_tokens(tokens):\n",
    "    # Check if the tokens are a part of the vocabulary\n",
    "    ...\n",
    "\n",
    "def generate_answer(question: str) -> str:\n",
    "    # Generate an answer for the given question\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*app.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from telegram import Update\n",
    "from telegram.ext import ApplicationBuilder, ContextTypes, CommandHandler, MessageHandler\n",
    "from dotenv import dotenv_values\n",
    "from warnings import filterwarnings\n",
    "\n",
    "from utils import generate_answer\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "\n",
    "async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
    "    # Response for the /start command\n",
    "    ...\n",
    "\n",
    "async def respond(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
    "    # Handle user messages\n",
    "    ...\n",
    "\n",
    "\n",
    "app = ApplicationBuilder().token(config['TELEGRAM_TOKEN']).build()\n",
    "app.add_handler(CommandHandler(callback=start, command='start'))\n",
    "app.add_handler(MessageHandler(callback=respond, filters=None))\n",
    "app.run_polling()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "\n",
    "Handle potential errors that may occur, such as model errors or invalid inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The app first splits the message into sentences. The bot will respond to each sentence separately.\n",
    "2. After tokenizing the sentences, the tokens are checked if they are in the vocabulary.\n",
    "3. For any uncaught exception, the bot will respond with a default error message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collaborations\n",
    "\n",
    "1. *Beatricia Golban* FAF-213 - helped with the model implementation.\n",
    "2. *Dan Hariton* FAF-211 - helped with the model and bot implementation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
