{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Fundamentals of Artificial Intelligence***\n",
    "\n",
    "> **Lab 6:** *Natural Language Processing and Chat Bots* <br>\n",
    "\n",
    "> **Performed by:** *Corneliu Catlabuga*, group *FAF-213* <br>\n",
    "\n",
    "> **Verified by:** Elena Graur, asist. univ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "\n",
    "Set up the Telegram Bot. Interact with BotFather on Telegram to obtain an API token. Create your Telegram Bot (its name should follow the pattern FIA_Surname_Name_FAF_21x). Make sure you are able to receive and send requests to it.\n",
    "\n",
    "1. Bot link: [FIA_Catlabuga_Corneliu_FAF_213](https://t.me/FAFCatlabugaCorneliuFAF213bot)\n",
    "\n",
    "2. Run `app.py` to start the bot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "\n",
    "Create a dataset that will serve as a training set for your model. It should follow the rules:\n",
    "- an entry consists of two parts: the question and the answer;\n",
    "- there are at least 75 entries written by you in your dataset;\n",
    "- questions should be something tourists or locals can ask about a new city.\n",
    "\n",
    "You can increase your dataset by adding open-source data. However, you MUST clearly show the questions written by you. Split your dataset into train and validation.\n",
    "\n",
    "*Hint: it is recommended to split it into 80% and 20%, but you can adjust it according to your needs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset.csv')\n",
    "train, val = train_test_split(dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "\n",
    "Use Tensorflow or Pytorch to implement the architecture of the Neural Network you are planning to use. It is highly recommended to use a Seq2Seq model (implement an LSTM or GRU architecture). You are NOT allowed to use pre-built or existing solutions (yep, connecting to GPT will not work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, source_len, target_len, source_text, target_text):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = target_len\n",
    "        self.target_text = self.data[target_text]\n",
    "        self.source_text = self.data[source_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_text = str(self.source_text[index])\n",
    "        target_text = str(self.target_text[index])\n",
    "\n",
    "        source_text = ' '.join(source_text.split())\n",
    "        target_text = ' '.join(target_text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([source_text], max_length= self.source_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([target_text], max_length= self.summ_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long),\n",
    "            'source_mask': source_mask.to(dtype=torch.long),\n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "\n",
    "Train your model and fine-tune it based on the chosen performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, loader, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    train_loop = tqdm(enumerate(loader, 0), leave = True)\n",
    "\n",
    "    for _, data in train_loop:\n",
    "        train_loop.set_description(f\"Iteration {_}\")\n",
    "\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loop.set_postfix(loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids = ids,\n",
    "                attention_mask = mask,\n",
    "                max_length=150,\n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.5,\n",
    "                length_penalty=1.0,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "\n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T5Trainer(dataframe, source_text, target_text, model_params, output_dir=\"./outputs/\" ):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "    model = model.to(device)\n",
    "\n",
    "    dataframe = dataframe[[source_text,target_text]]\n",
    "\n",
    "    train_size = 0.8\n",
    "    train_dataset=dataframe.sample(frac=train_size,random_state = model_params[\"SEED\"])\n",
    "    val_dataset=dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    training_set = Seq2SeqDataset(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "    val_set = Seq2SeqDataset(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "\n",
    "    train_params = {\n",
    "        'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0\n",
    "    }\n",
    "\n",
    "    val_params = {\n",
    "        'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
    "        'shuffle': False,\n",
    "        'num_workers': 0\n",
    "        }\n",
    "\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
    "\n",
    "    loop = tqdm(range(model_params[\"TRAIN_EPOCHS\"]), leave=True)\n",
    "\n",
    "    for epoch in loop:\n",
    "        train(epoch, tokenizer, model, training_loader, optimizer)\n",
    "\n",
    "    path = os.path.join(output_dir, \"model_files\")\n",
    "    model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)\n",
    "\n",
    "    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "        predictions, actuals = validate(epoch, tokenizer, model, val_loader)\n",
    "        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "        final_df.to_csv(os.path.join(output_dir,'predictions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 8: : 9it [02:54, 19.38s/it, loss=tensor(6.3262, device='cuda:0', grad_fn=<NllLossBackward0>)]\n",
      "100%|██████████| 1/1 [02:54<00:00, 174.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed...\n"
     ]
    }
   ],
   "source": [
    "model_params={\n",
    "    \"MODEL\":\"t5-base\",             # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\": 8,         # training batch size\n",
    "    \"VALID_BATCH_SIZE\": 8,         # validation batch size\n",
    "    \"TRAIN_EPOCHS\": 1,             # number of training epochs\n",
    "    \"VAL_EPOCHS\": 1,               # number of validation epochs\n",
    "    \"LEARNING_RATE\": 1e-4,         # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\": 512, # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\": 50,  # max length of target text\n",
    "    \"SEED\": 69                     # set seed for reproducibility\n",
    "}\n",
    "\n",
    "T5Trainer(dataframe=dataset, source_text=\"question\", target_text=\"answer\", model_params=model_params, output_dir=\"models/output0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "\n",
    "Integrate your model into your Telegram ChatBot, so that the sent messages are taken as input by the model and its output is sent back as a reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reponse():\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "\n",
    "Handle potential errors that may occur, such as model errors or invalid inputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
