{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Fundamentals of Artificial Intelligence***\n",
    "\n",
    "> **Lab 6:** *Natural Language Processing and Chat Bots* <br>\n",
    "\n",
    "> **Performed by:** *Corneliu Catlabuga*, group *FAF-213* <br>\n",
    "\n",
    "> **Verified by:** Elena Graur, asist. univ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from warnings import filterwarnings\n",
    "\n",
    "MODEL = 't5-base'\n",
    "SOURCE_LEN = 512\n",
    "TARGET_LEN = 512\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "\n",
    "Set up the Telegram Bot. Interact with BotFather on Telegram to obtain an API token. Create your Telegram Bot (its name should follow the pattern FIA_Surname_Name_FAF_21x). Make sure you are able to receive and send requests to it.\n",
    "\n",
    "1. Bot link: [FIA_Catlabuga_Corneliu_FAF_213](https://t.me/fiacorneliucatlabugafaf213bot)\n",
    "\n",
    "2. Run `app.py` to start the bot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "\n",
    "Create a dataset that will serve as a training set for your model. It should follow the rules:\n",
    "- an entry consists of two parts: the question and the answer;\n",
    "- there are at least 75 entries written by you in your dataset;\n",
    "- questions should be something tourists or locals can ask about a new city.\n",
    "\n",
    "You can increase your dataset by adding open-source data. However, you MUST clearly show the questions written by you. Split your dataset into train and validation.\n",
    "\n",
    "*Hint: it is recommended to split it into 80% and 20%, but you can adjust it according to your needs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset.csv')\n",
    "\n",
    "questions = dataset['question'].tolist()\n",
    "answers = dataset['answer'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "\n",
    "Use Tensorflow or Pytorch to implement the architecture of the Neural Network you are planning to use. It is highly recommended to use a Seq2Seq model (implement an LSTM or GRU architecture). You are NOT allowed to use pre-built or existing solutions (yep, connecting to GPT will not work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "def tokenize(data, max_len):\n",
    "    return tokenizer(data, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs['input_ids'][idx],\n",
    "            'attention_mask': self.inputs['attention_mask'][idx],\n",
    "            'labels': self.targets['input_ids'][idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_questions = tokenize(questions, SOURCE_LEN)\n",
    "# tokenized_answers = tokenize(answers, TARGET_LEN)\n",
    "\n",
    "# dataset = Seq2SeqDataset(tokenized_questions, tokenized_answers)\n",
    "# dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "_, questions_val, _, answers_val = train_test_split(questions, answers, test_size=0.2)\n",
    "\n",
    "tokenized_questions_train = tokenize(questions, SOURCE_LEN)\n",
    "tokenized_answers_train = tokenize(answers, TARGET_LEN)\n",
    "train_dataset = Seq2SeqDataset(tokenized_questions_train, tokenized_answers_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "tokenized_questions_val = tokenize(questions_val, SOURCE_LEN)\n",
    "tokenized_answers_val = tokenize(answers_val, TARGET_LEN)\n",
    "val_dataset = Seq2SeqDataset(tokenized_questions_val, tokenized_answers_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "\n",
    "Train your model and fine-tune it based on the chosen performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, model_name=MODEL):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs: int = 10, file_name: str = 'model.pth'):\n",
    "    model = Seq2SeqModel().to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loop = tqdm(train_dataloader, leave=True)\n",
    "\n",
    "        for batch in loop:\n",
    "            loop.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    torch.save(model.state_dict(), f'models/{file_name}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    total_bleu = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(val_loader, leave=True)\n",
    "        for batch in loop:\n",
    "            loop.set_description(\"Evaluating: \")\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model.model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=TARGET_LEN)\n",
    "            predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            references = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            for p, r in zip(predictions, references):\n",
    "                total_bleu += sentence_bleu([r.split()], p.split())\n",
    "                num_samples += 1\n",
    "\n",
    "    return total_bleu / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 8/8 [00:15<00:00,  1.94s/it, loss=2.3] \n",
      "Epoch 1: 100%|██████████| 8/8 [00:13<00:00,  1.71s/it, loss=1.35]\n",
      "Epoch 2: 100%|██████████| 8/8 [00:08<00:00,  1.10s/it, loss=1.11]\n",
      "Epoch 3: 100%|██████████| 8/8 [00:09<00:00,  1.22s/it, loss=0.859]\n",
      "Epoch 4: 100%|██████████| 8/8 [00:09<00:00,  1.22s/it, loss=0.729]\n",
      "Epoch 5: 100%|██████████| 8/8 [00:09<00:00,  1.22s/it, loss=0.622]\n",
      "Epoch 6: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it, loss=0.399]\n",
      "Epoch 7: 100%|██████████| 8/8 [00:09<00:00,  1.22s/it, loss=0.264]\n",
      "Epoch 8: 100%|██████████| 8/8 [00:09<00:00,  1.22s/it, loss=0.259]\n",
      "Epoch 9: 100%|██████████| 8/8 [00:10<00:00,  1.26s/it, loss=0.228]\n",
      "Evaluating: : 100%|██████████| 2/2 [00:00<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.6664665989118275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TRAINED_MODEL = 'model10base.pth'\n",
    "\n",
    "model = train(10, TRAINED_MODEL)\n",
    "bleu_score = evaluate(model, val_dataloader)\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "\n",
    "Integrate your model into your Telegram ChatBot, so that the sent messages are taken as input by the model and its output is sent back as a reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The loneibus runs every 10 minads during peak hours and every 20 minads otherwise.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Seq2SeqModel().to(device)\n",
    "model.load_state_dict(torch.load(f'models/{TRAINED_MODEL}'))\n",
    "\n",
    "def generate_answer(question, model, tokenizer):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "    output = model.model.generate(input_ids, max_length=TARGET_LEN)\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return decoded\n",
    "\n",
    "# Test example\n",
    "test_question = \"How often do loneibus buses run?\"\n",
    "display(generate_answer(test_question, model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "\n",
    "Handle potential errors that may occur, such as model errors or invalid inputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
